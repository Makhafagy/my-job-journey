# ðŸš€ An Automated Application Tracker

This project provides a suite of Python and Google Apps scripts to automate the process of finding, filtering, and analyzing new-graduate software engineering job applications. It transforms the job hunt from a chaotic spreadsheet nightmare into a streamlined, data-driven workflow.

This system scrapes the latest job postings, filters them against your application history, and generates performance reports on your entire application funnelâ€”showing you exactly how your efforts are translating into interviews and offers over time.

---

## âœ¨ Key Features

* **Automated Scraping:** Pulls the latest new-grad software engineering roles from the SimplifyJobs `New-Grad-Positions` GitHub repository.
* **Intelligent Filtering:** Automatically filters scraped jobs to show only the most relevant opportunities based on location (US-only), age (posted < 7 days), and your complete application history.
* **Google Sheets Integration:** Includes a Google Apps Script to seamlessly manage your tracking sheet, automatically adding checkboxes and highlighting applied-to jobs.
* **Historical Analysis:** Aggregates data from your current application file and an archive of past files (`past_applied_data`) to provide a complete, all-time view of your job search performance.
* **Performance Analytics:** The analysis script calculates key performance indicators (KPIs), including your Interview Rate, Offer Rate, and Ghosted Rate.

---

## âš™ï¸ Project Components

The project is composed of several scripts and key data files that work together.

### The Scripts

* **`pull_apply_links.py`**: The main scraper that fetches all active job listings and performs initial filtering.
* **`filter_jobs.py`**: Cross-references the newly scraped jobs against your entire application history (current and past files) and removes any you have already applied to.
* **`prepare_tracker.py`**: A one-time setup script that adds the necessary tracking columns (`Date Applied`, `Status`) to a new master CSV file.
* **`detailed_analysis.py`**: The analytics engine. It reads your current and past application files, calculates your funnel metrics, and saves a performance report.
* **`count_applications.py`**: A simpler analysis script that provides a quick count and company breakdown of your total applications over time.
* **`google_sheets_tracker.gs`**: The Google Apps Script code used to manage your tracking spreadsheet.

### The Data Files & Folders (Ignored by Git)

* **`new_grad_swe_apply_links.csv`**: A temporary file containing the fresh, filtered list of jobs to import into Google Sheets.
* **`new_grad_swe_apply_links_applying.csv`**: Your **current** master tracking file, downloaded from Google Sheets.
* **`past_applied_data/`**: A folder containing all your **archived** application files from previous sessions.
* **`application_analysis.csv`**: The output report generated by the analysis script.

---

## ðŸ“ˆ How to Use: A Step-by-Step Guide

### Part 1: Initial Setup (One-Time Only)

1.  **Install Prerequisites:** Make sure you have Python 3 installed. Then, install the required libraries from your terminal:
    ```bash
    pip install requests beautifulsoup4
    ```
2.  **Create Your Folders:** Make sure you have a `past_applied_data` folder in your project directory.
3.  **Set Up Google Sheets:**
    * Create a new Google Sheet.
    * Go to `Extensions > Apps Script`.
    * Delete any placeholder code and paste the entire content from the `google_sheets_tracker.gs` file into the editor.
    * Save the script project. You will now have an "Apply Tracker" menu in this sheet.

### Part 2: The Main Workflow

Perform these steps whenever you want to find and apply for new jobs.

1.  **Find New Jobs:** Run this script in your project folder to generate an up-to-date list of job openings.
    ```bash
    # Get the latest jobs scraped from simplify GitHub (default: up to 7 days of posting age)
    python pull_apply_links.py

    # Use the optional --days argument to control how far back the job search goes. If the below is omitted, it'll scrape postings up to 3 days of posting age.
    python pull_apply_links.py --days 3
    ```

2.  **Track in Google Sheets:**
    * **Import Data:** In your Google Sheet, go to `File > Import` and upload the newly generated `new_grad_swe_apply_links.csv` file. Select "Replace current sheet".
    * **Prepare Tracker:** Use your custom `Apply Tracker > Add Applied Column` menu to add the checkbox column.
    * **Apply and Check the Box:** As you apply to jobs using the links in the sheet, simply check the box in the `Applied` column for that row.

3.  **Update Local Files for Analysis:** Once you are done applying for a session:
    * **Download from Sheets:** Download your current Google Sheet as a CSV file (`File > Download > Comma Separated Values (.csv)`).
    * **Archive the Old File:** If you have an existing `new_grad_swe_apply_links_applying.csv` file, rename it (e.g., to `applications_sept_2025.csv`) and move it into the `past_applied_data` folder.
    * **Name the New File:** Name the file you just downloaded exactly **`new_grad_swe_apply_links_applying.csv`** and place it in the main project folder.

4.  **Prepare Local File & Run Analysis:**
    * Prepare Tracker File: Run this to ensure your downloaded master file has the necessary Date Applied and Status columns for analysis.
        ```bash
        python prepare_tracker.py
        ```
    * Run the analysis script to see your updated, all-time statistics.
        ```bash
        python detailed_analysis.py
        ```
    * As you hear back from companies, update the **`Status`** column in your Google Sheet with values like `Interview`, `Offer`, etc., and then repeat Step 3 to see your new funnel metrics.

---
