# 🚀 An Automated Application Tracker

This project provides a suite of Python and Google Apps scripts to automate the process of finding, filtering, and analyzing new-graduate software engineering job applications. It transforms the job hunt from a chaotic spreadsheet nightmare into a streamlined, data-driven workflow.

This system scrapes the latest job postings, filters them against your application history, and generates performance reports on your entire application funnel—showing you exactly how your efforts are translating into interviews and offers over time.

---

## ✨ Key Features

* **Automated Scraping:** Pulls the latest new-grad software engineering roles from the SimplifyJobs `New-Grad-Positions` GitHub repository.
* **Intelligent Filtering:** Automatically filters scraped jobs to show only the most relevant opportunities based on location (US-only), age (posted < 7 days), and your complete application history.
* **Google Sheets Integration:** Includes a Google Apps Script to seamlessly manage your tracking sheet, automatically adding checkboxes and highlighting applied-to jobs.
* **Historical Analysis:** Aggregates data from your current application file and an archive of past files (`past_applied_data`) to provide a complete, all-time view of your job search performance.
* **Performance Analytics:** The analysis script calculates key performance indicators (KPIs), including your Interview Rate, Offer Rate, and Ghosted Rate.

---

## ⚙️ Project Components

The project is composed of several scripts and key data files that work together.

### The Scripts

* **`pull_apply_links.py`**: The main scraper that fetches all active job listings and performs initial filtering.
* **`compare_applied.py`**: Cross-references the newly scraped jobs against your entire application history (current and past files in `past_applied_data`) and removes any you have already applied to. It overwrites `new_grad_swe_apply_links.csv` with only fresh jobs.
* **`prepare_tracker.py`**: A one-time setup script that adds the necessary tracking column (`Status`) to a new master CSV file.
* **`detailed_analysis.py`**: The analytics engine. It reads your current and past application files, calculates your funnel metrics, and saves a performance report.
* **`count_applications.py`**: A simpler analysis script that provides a quick count and company breakdown of your total applications over time.
* **`google_sheets_tracker.gs`**: The Google Apps Script code used to manage your tracking spreadsheet.

### The Data Files & Folders (Ignored by Git)

* **`new_grad_swe_apply_links.csv`**: A file containing the fresh, filtered list of jobs to import into Google Sheets.
* **`past_applied_data/`**: A folder containing all your **archived** application files from previous sessions (`new_grad_swe_apply_links_applying_1.csv`, `new_grad_swe_apply_links_applying_2.csv`, etc.).
* **`application_analysis.csv`**: The output report generated by the analysis script.

---

## 📈 How to Use: A Step-by-Step Guide

### Part 1: Initial Setup (One-Time Only)

1.  **Install Prerequisites:** Make sure you have Python 3 installed. Then, install the required libraries from your terminal:
    ```bash
    pip install requests beautifulsoup4
    ```
2.  **Create Your Folders:** Make sure you have a `past_applied_data` folder in your project directory.
3.  **Set Up Google Sheets:**
    * Create a new Google Sheet.
    * Go to `Extensions > Apps Script`.
    * Delete any placeholder code and paste the entire content from the `google_sheets_tracker.gs` file into the editor.
    * Save the script project. You will now have an "Apply Tracker" menu in this sheet.

### Part 2: The Main Workflow

Perform these steps whenever you want to find and apply for new jobs.

1.  **Find New Jobs:** Run this script in your project folder to generate an up-to-date list of job openings.
    ```bash
    # Get the latest jobs scraped from simplify GitHub (default: up to 7 days of posting age)
    python pull_apply_links.py

    # Use the optional --days argument to control how far back the job search goes. 
    # Example: only scrape postings up to 3 days of posting age.
    python pull_apply_links.py --days 3
    ```

2.  **Filter Out Applied Jobs:** Run this to automatically remove jobs you've already applied to (based on files in `past_applied_data`). This updates `new_grad_swe_apply_links.csv` in place.
    ```bash
    python compare_applied.py
    ```

3.  **Track in Google Sheets:**
    * **Import Data:** In your Google Sheet, go to `File > Import` and upload the newly generated `new_grad_swe_apply_links.csv` file. Select "Replace current sheet".
    * **Prepare Tracker:** Use your custom `Apply Tracker > Add Status Column` menu to add the status column if it's missing.
    * **Apply and Update Status:** As you apply to jobs using the links in the sheet, update the `Status` column for that row (e.g., `Applied`, `Interview`, `Offer`).

4.  **Update Local Files for Analysis:** Once you are done applying for a session:
    * **Download from Sheets:** Download your current Google Sheet as a CSV file (`File > Download > Comma Separated Values (.csv)`).
    * **Archive the downloaded sheet:**  Move it into the `past_applied_data` folder, and rename it (to `new_grad_swe_apply_links_applying_1.csv`) and increment filename as you archive more.

5.  **Run Analysis & Update Status:**
    * Run the analysis script to see your updated, all-time statistics.
        ```bash
        python detailed_analysis.py
        ```
    * As you hear back from companies, keep updating the **`Status`** column in your Google Sheet and repeat Step 4 to see your new funnel metrics.

---
