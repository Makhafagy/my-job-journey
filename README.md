# ðŸš€ An Automated Application Tracker

This project provides a suite of Python and Google Apps scripts to automate the process of finding, filtering, and analyzing new-graduate software engineering job applications. It transforms the job hunt from a chaotic spreadsheet nightmare into a streamlined, data-driven workflow.

This system scrapes the latest job postings, filters them against your application history, and generates performance reports on your entire application funnelâ€”showing you exactly how your efforts are translating into interviews and offers over time.

---

## âœ¨ Key Features

* **Automated Scraping:** Pulls the latest new-grad software engineering roles from the SimplifyJobs `New-Grad-Positions` GitHub repository.
* **Intelligent Filtering:** Automatically filters scraped jobs to show only the most relevant opportunities based on location (US-only), age (posted < 7 days), and your complete application history.
* **Google Sheets Integration:** Includes a Google Apps Script to seamlessly manage your tracking sheet, automatically adding checkboxes and highlighting applied-to jobs.
* **Historical Analysis:** Aggregates data from your current application file and an archive of past files (`past_applied_data`) to provide a complete, all-time view of your job search performance.
* **Performance Analytics:** The analysis script calculates key performance indicators (KPIs), including your Interview Rate, Offer Rate, and Ghosted Rate.

---

## âš™ï¸ Project Components

The project is composed of several scripts and key data files that work together.

### The Scripts

* **`pull_apply_links.py`**: The main scraper that fetches all active job listings and performs initial filtering.
* **`compare_applied.py`**: Cross-references the newly scraped jobs against your entire application history (current and past files in `past_applied_data`) and removes any you have already applied to. It overwrites `new_grad_swe_apply_links.csv` with only fresh jobs.
* **`prepare_tracker.py`**: A one-time setup script that adds the necessary tracking column (`Status`) to a new master CSV file.
* **`detailed_analysis.py`**: The analytics engine. It reads your current and past application files, calculates your funnel metrics, and saves a performance report.
* **`count_applications.py`**: A simpler analysis script that provides a quick count and company breakdown of your total applications over time.
* **`google_sheets_tracker.gs`**: The Google Apps Script code used to manage your tracking spreadsheet.

### The Data Files & Folders (Ignored by Git)

* **`new_grad_swe_apply_links.csv`**: A file containing the fresh, filtered list of jobs to import into Google Sheets.
* **`past_applied_data/`**: A folder containing all your **archived** application files from previous sessions (`new_grad_swe_apply_links_applying_1.csv`, `new_grad_swe_apply_links_applying_2.csv`, etc.).
* **`application_analysis.csv`**: The output report generated by the analysis script.

---

## ðŸ“ˆ How to Use: A Step-by-Step Guide

### Part 1: Initial Setup (One-Time Only)

1.  **Install Prerequisites:** Make sure you have Python 3 installed. Then, install the required libraries from your terminal:
    ```bash
    pip install requests beautifulsoup4
    ```
2.  **Create Your Folders:** Make sure you have a `past_applied_data` folder in your project directory.
3.  **Set Up Google Sheets:**
    * Create a new Google Sheet.
    * Go to `Extensions > Apps Script`.
    * Delete any placeholder code and paste the entire content from the `google_sheets_tracker.gs` file into the editor.
    * Save the script project. You will now have an "Apply Tracker" menu in this sheet.

### Part 2: The Main Workflow

Perform these steps whenever you want to find and apply for new jobs.

1.  **Find New Jobs:** Run this script in your project folder to generate an up-to-date list of job openings.
    ```bash
    # Get the latest jobs scraped from simplify GitHub (default: up to 7 days of posting age)
    python pull_apply_links.py

    # Use the optional --days argument to control how far back the job search goes. 
    # Example: only scrape postings up to 3 days of posting age.
    python pull_apply_links.py --days 3
    ```

2.  **Filter Out Applied Jobs:** Run this to automatically remove jobs you've already applied to (based on files in `past_applied_data`). This updates `new_grad_swe_apply_links.csv` in place.
    ```bash
    python compare_applied.py
    ```

3.  **Track in Google Sheets:**
    * **Import Data:** In your Google Sheet, go to `File > Import` and upload the newly generated `new_grad_swe_apply_links.csv` file. Select "Replace current sheet".
    * **Prepare Tracker:** Use your custom `Apply Tracker > Add Status Column` menu to add the status column if it's missing.
    * **Apply and Update Status:** As you apply to jobs using the links in the sheet, update the `Status` column for that row (e.g., `Applied`, `Interview`, `Offer`).

4.  **Update Local Files for Analysis:** Once you are done applying for a session:
    * **Download from Sheets:** Download your current Google Sheet as a CSV file (`File > Download > Comma Separated Values (.csv)`).
    * **Archive the downloaded sheet:**  Move it into the `past_applied_data` folder, and rename it (to `new_grad_swe_apply_links_applying_1.csv`) and increment filename as you archive more.

5.  **Run Analysis & Update Status:**
    * Run the analysis script to see your updated, all-time statistics.
        ```bash
        python detailed_analysis.py
        ```
    * As you hear back from companies, keep updating the **`Status`** column in your Google Sheet and repeat Step 4 to see your new funnel metrics.

---
